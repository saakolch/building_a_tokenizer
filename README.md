# building_a_tokenizer
A tokenizer based on WikiText-2
The primary purpose of this work is creating a tokenization alghorithm like BERT, GPT-2, XLNet.
The tikenizer is based on wiki text, which you can find here: https://huggingface.co/datasets/wikitext
